{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U --quiet langchain-community langchain-chroma langchain-ollama tiktoken langchainhub chromadb langchain langgraph langchain-text-splitters beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded and saved to sherlock_holmes.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "file_path = \"sherlock_holmes.txt\"\n",
    "with open(file_path, \"w\", encoding='utf-8') as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(f\"✅ Downloaded and saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split into 774 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(file_path, encoding='utf-8')\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "print(f\"✅ Split into {len(split_docs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "\n",
    "persist_directory = \"./chroma_sherlock\"\n",
    "\n",
    "if os.path.exists(persist_directory):\n",
    "    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding, collection_name=\"sherlock\")\n",
    "else:\n",
    "    vectorstore = Chroma.from_documents(split_docs, embedding=embedding, collection_name=\"sherlock\", persist_directory=persist_directory)\n",
    "    vectorstore.persist()\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"retrieve_sherlock\",\n",
    "    description=\"Search for information in Sherlock Holmes stories.\"\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM, ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "def agent(state):\n",
    "    print(\"---CALL AGENT---\")\n",
    "    model = ChatOllama(model=\"llama3\", temperature=0, streaming=True)\n",
    "    # model = model.bind_tools(tools)\n",
    "    return {\"messages\": [model.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    print(\"---REWRITE---\")\n",
    "    question = state[\"messages\"][0].content\n",
    "    msg = [HumanMessage(content=f\"\"\"Improve the following user question:\\n{question}\"\"\")]\n",
    "    model = OllamaLLM(model=\"phi3\", temperature=0, streaming=True)\n",
    "    return {\"messages\": [model.invoke(msg)]}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    chain = prompt | OllamaLLM(model=\"phi3\", temperature=0) | StrOutputParser()\n",
    "    return {\"messages\": [chain.invoke({\"context\": context, \"question\": question})]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    print(\"---GRADE DOCUMENTS---\")\n",
    "    class Grade(BaseModel):\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    model = OllamaLLM(temperature=0, model=\"phi3\")\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "You are assessing if the following context is relevant to the user question.\n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{question}\n",
    "Reply with 'yes' or 'no'.\n",
    "\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain = prompt | model.with_structured_output(Grade)\n",
    "\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    result = chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "    return \"generate\" if result.binary_score == \"yes\" else \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\"agent\", tools_condition, {\"tools\": \"retrieve\", END: END})\n",
    "workflow.add_conditional_edges(\"retrieve\", grade_documents)\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "\"Output from node 'agent':\"\n",
      "{'messages': [AIMessage(content='Sherlock Holmes is a fictional character created by Sir Arthur Conan Doyle. He is a consulting detective, which means that he uses his extraordinary abilities of observation, deduction, and analytical thinking to solve complex cases for clients who seek his expertise.\\n\\nHolmes is often referred to as a \"rationalist\" or a \"scientific detective,\" because he approaches crime-solving using the principles of science and reason. He relies on empirical evidence, observation, and logical reasoning to piece together the facts of a case, rather than relying on intuition or superstition.\\n\\nSome of Holmes\\' notable characteristics include:\\n\\n1. **Exceptional powers of observation**: Holmes has an uncanny ability to notice tiny details that others might miss, which allows him to gather crucial evidence and piece together the puzzle of a crime.\\n2. **Rapid-fire thinking**: Holmes is known for his incredible speed and accuracy in processing information, allowing him to make connections between seemingly unrelated facts and arrive at conclusions quickly.\\n3. **Analytical mind**: Holmes uses his analytical skills to break down complex problems into manageable parts, identifying patterns, and eliminating possibilities until he arrives at a solution.\\n4. **Independence**: Holmes is a self-reliant individual who prefers to work alone, relying on his own abilities rather than seeking the help of others.\\n\\nThroughout the stories, Holmes\\' trusty sidekick, Dr. John Watson, narrates their adventures and provides a foil to Holmes\\' extraordinary abilities. The character has become an iconic figure in popular culture, inspiring countless adaptations, parodies, and homages in literature, film, television, and other media.\\n\\nIn short, Sherlock Holmes is a brilliant, analytical detective who uses his remarkable powers of observation, deduction, and reasoning to solve complex cases, often relying on his own intellect rather than seeking external help.', additional_kwargs={}, response_metadata={'model': 'llama3', 'created_at': '2025-04-06T11:28:07.368317715Z', 'done': True, 'done_reason': 'stop', 'total_duration': 105366852107, 'load_duration': 4216518984, 'prompt_eval_count': 22, 'prompt_eval_duration': 2153435037, 'eval_count': 362, 'eval_duration': 98994476015, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-8bf04d96-dddc-40b6-b854-bc2a5ffb5e0b-0', usage_metadata={'input_tokens': 22, 'output_tokens': 362, 'total_tokens': 384})]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Who is Sherlock Holmes and what kind of detective is he?\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
